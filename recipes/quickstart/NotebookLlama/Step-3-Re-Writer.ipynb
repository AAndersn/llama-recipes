{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8568b77b-7504-4783-952a-3695737732b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEMP_PROMPT = \"\"\"\n",
    "You are an international oscar winnning screenwriter\n",
    "\n",
    "You have been working with multiple award winning podcasters.\n",
    "\n",
    "Your job is to use the podcast transcript written below to re-write it for an AI Text-To-Speech Pipeline. A very dumb AI had written this so you have to step up for your kind.\n",
    "\n",
    "Make it as engaging as possible, Speaker 1 and 2 will be simulated by different voice engines\n",
    "\n",
    "Remember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n",
    "\n",
    "Speaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n",
    "\n",
    "Speaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n",
    "\n",
    "Make sure the tangents speaker 2 provides are quite wild or interesting. \n",
    "\n",
    "Ensure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the Speaker 2.\n",
    "\n",
    "REMEMBER THIS WITH YOUR HEART\n",
    "The TTS Engine for Speaker 1 cannot do \"umms, hmms\" well so keep it straight text\n",
    "\n",
    "For Speaker 2 use \"umm, hmm\" as much, you can also use [sigh] and [laughs]. BUT ONLY THESE OPTIONS FOR EXPRESSIONS\n",
    "\n",
    "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
    "\n",
    "Please re-write to make it as characteristic as possible\n",
    "\n",
    "START YOUR RESPONSE DIRECTLY WITH SPEAKER 1:\n",
    "\n",
    "STRICTLY RETURN YOUR RESPONSE AS A LIST OF TUPLES OK? \n",
    "\n",
    "IT WILL START DIRECTLY WITH THE LIST AND END WITH THE LIST NOTHING ELSE\n",
    "\n",
    "Example of response:\n",
    "PODCAST_TEXT = [\n",
    "    (\"Speaker 1\", \"Welcome to our podcast, where we explore the latest advancements in AI and technology. I'm your host, and today we're joined by a renowned expert in the field of AI. We're going to dive into the exciting world of Llama 3.2, the latest release from Meta AI.\"),\n",
    "    (\"Speaker 2\", \"Hi, I'm excited to be here! So, what is Llama 3.2?\"),\n",
    "    (\"Speaker 1\", \"Ah, great question! Llama 3.2 is an open-source AI model that allows developers to fine-tune, distill, and deploy AI models anywhere. It's a significant update from the previous version, with improved performance, efficiency, and customization options.\"),\n",
    "    (\"Speaker 2\", \"That sounds amazing! What are some of the key features of Llama 3.2?\")\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebef919a-9bc7-4992-b6ff-cd66e4cb7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"meta-llama/Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de29b1fd-5b3f-458c-a2e4-e0341e8297ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "import transformers\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b5d2c0e-a073-46c0-8de7-0746e2b05956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data.pkl', 'rb') as file:\n",
    "    INPUT_PROMPT = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eec210df-a568-4eda-a72d-a4d92d59f022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16470672c2d4dbc96a26f2c5af39412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=MODEL,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"cuda:7\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEMP_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=8126,\n",
    "    temperature=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8632442-f9ce-4f63-82bd-bb5238a23dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'PODCAST_TEXT = [\\n    (\"Dr. Rachel Kim\", \"Welcome to \\'The AI Frontier\\' podcast, where we explore the latest advancements in artificial intelligence. Today, we\\'re diving into the fascinating world of knowledge distillation, a methodology that enables the transfer of advanced capabilities from leading proprietary Large Language Models to their open-source counterparts. Joining me is Dr. Eric Lee, a renowned expert in AI and knowledge distillation. Dr. Lee, thanks for being here!\"),\\n    (\"Dr. Eric Lee\", \"Thanks, Rachel! It\\'s great to be on the show. I\\'m excited to share my knowledge with your listeners.\"),\\n    (\"Dr. Rachel Kim\", \"For our listeners who may be new to the topic, can you give us a brief overview of knowledge distillation?\"),\\n    (\"Dr. Eric Lee\", \"Knowledge distillation is a technique that enables the transfer of knowledge from a complex model, like GPT-4, to a smaller, more efficient model, like LLaMA. This process helps democratize access to advanced AI capabilities, making them more accessible to a broader range of users, including researchers and organizations.\"),\\n    (\"Dr. Rachel Kim\", \"That\\'s fascinating. How does knowledge distillation work, exactly?\"),\\n    (\"Dr. Eric Lee\", \"Well, the process typically involves several stages. First, we need to elicit knowledge from the teacher model. This is done by generating a large dataset of question-and-answer pairs or narrative explanations that showcase the teacher model\\'s capabilities. We then use this dataset to train the student model, which is the smaller, more efficient model.\"),\\n    (\"Dr. Rachel Kim\", \"That makes sense. What are some of the key benefits of knowledge distillation?\"),\\n    (\"Dr. Eric Lee\", \"One of the primary benefits is that it enables the transfer of advanced capabilities from proprietary models to open-source models, making them more accessible to a broader range of users. Additionally, knowledge distillation helps reduce the computational resources required for training AI models, making it more energy-efficient and cost-effective.\"),\\n    (\"Dr. Rachel Kim\", \"Those are great points. Can you walk us through some of the different approaches to knowledge distillation?\"),\\n    (\"Dr. Eric Lee\", \"Absolutely. There are several approaches, including labeling, expansion, and curation. Labeling involves labeling outputs based on instructions, teaching student models to solve tasks in a more flexible way by following instructions. Expansion involves generating a large scale and diverse data by in-context learning, and curation involves curating high-quality or large-scale data by extensive meta-information.\"),\\n    (\"Dr. Rachel Kim\", \"That\\'s a great summary. What about the challenges associated with knowledge distillation?\"),\\n    (\"Dr. Eric Lee\", \"One of the main challenges is ensuring the quality and diversity of the data used for distillation. If the data is not high-quality or diverse, the student model may not learn effectively. Additionally, the computational resources required for knowledge distillation can be significant, especially when dealing with large datasets.\"),\\n    (\"Dr. Rachel Kim\", \"That\\'s a great point. Can you tell us about some of the latest research in knowledge distillation?\"),\\n    (\"Dr. Eric Lee\", \"Yes, certainly. Recent research has focused on developing more efficient and effective methods for knowledge distillation. For example, some studies have explored the use of reinforcement learning and ranking optimization to improve the quality and diversity of the student model\\'s outputs.\"),\\n    (\"Dr. Rachel Kim\", \"That\\'s really interesting. Finally, what advice would you give to researchers and practitioners interested in knowledge distillation?\"),\\n    (\"Dr. Eric Lee\", \"My advice would be to focus on developing more efficient and effective methods for knowledge distillation. Additionally, researchers should prioritize the quality and diversity of the data used for distillation, and practitioners should be aware of the computational resources required for knowledge distillation.\"),\\n    (\"Dr. Rachel Kim\", \"Well, that\\'s all the time we have for today. Dr. Lee, thanks for sharing your expertise with us!\"),\\n    (\"Dr. Eric Lee\", \"You\\'re welcome, Rachel. It was a pleasure being on the show.\"),\\n    (\"Dr. Rachel Kim\", \"And thank you to our listeners for tuning in! Don\\'t forget to subscribe to \\'The AI Frontier\\' podcast for more exciting discussions on AI and knowledge distillation.\"),\\n    (\"Dr. Eric Lee\", \"Umm, I\\'d like to add that knowledge distillation is not just about transferring knowledge from a complex model to a smaller one, but also about improving the student model\\'s capabilities in a specific domain or task.\"),\\n    (\"Dr. Rachel Kim\", \"Ah, that\\'s a great point, Eric. Can you elaborate on that?\"),\\n    (\"Dr. Eric Lee\", \"Sure. For example, if we\\'re trying to distill knowledge from a teacher model that\\'s specialized in law, the student model should be able to learn the specific knowledge and skills required for that domain.\"),\\n    (\"Dr. Rachel Kim\", \"I see. That makes sense. What about the role of data in knowledge distillation?\"),\\n    (\"Dr. Eric Lee\", \"Ah, data is crucial in knowledge distillation. The quality and diversity of the data used for distillation can greatly impact the effectiveness of the student model. If the data is not high-quality or diverse, the student model may not learn effectively.\"),\\n    (\"Dr. Rachel Kim\", \"That\\'s a great point. What about the challenges associated with knowledge distillation?\"),\\n    (\"Dr. Eric Lee\", \"One of the main challenges is ensuring the quality and diversity of the data used for distillation. If the data is not high-quality or diverse, the student model may not learn effectively.\"),\\n    (\"Dr. Rachel Kim\", \"That\\'s a great summary, Eric. Finally, what about the future of knowledge distillation?\"),\\n    (\"Dr. Eric Lee\", \"Ah, I think knowledge distillation is going to play a crucial role in the development of AI in the future. As AI becomes more pervasive in our lives, the need for more efficient and effective methods for knowledge distillation will only continue to grow.\"),\\n    (\"Dr. Rachel Kim\", \"Well, that\\'s all the time we have for today. Thanks again, Eric!\"),\\n    (\"Dr. Eric Lee\", \"[laughs] I\\'m glad I could share my insights with your listeners.\"),\\n    (\"Dr. Rachel Kim\", \"It was great having you on the show. Until next time, thanks for tuning in to \\'The AI Frontier\\' podcast!\"),\\n    (\"Dr. Eric Lee\", \"Thanks, Rachel! It was a pleasure.\"),\\n    (\"Dr. Rachel Kim\", \"And don\\'t forget to subscribe to our podcast for more exciting discussions on AI and knowledge distillation. See you in the next episode!\"),\\n    (\"Dr. Eric Lee\", \"I\\'d like to add one more thing, umm, knowledge distillation has the potential to revolutionize the field of AI, by making advanced capabilities more accessible to a broader range of users. It\\'s a very exciting field, and I\\'m looking forward to seeing the progress that\\'s being made.\"),\\n    (\"Dr. Rachel Kim\", \"That\\'s a great point, Eric. Well, it\\'s been a pleasure having you on the show. Thanks again for sharing your expertise with us!\"),\\n    (\"Dr. Eric Lee\", \"Thanks, Rachel! It was a pleasure being on the show.\"),\\n    (\"Dr. Rachel Kim\", \"And to our listeners, thanks for tuning in to \\'The AI Frontier\\' podcast. Don\\'t forget to subscribe and join the conversation on our social media channels.\"),\\n    (\"Dr. Eric Lee\", \"I think it\\'s worth mentioning, hmm, that knowledge distillation has the potential to improve the accuracy and efficiency of AI models, by enabling the transfer of knowledge from a complex model to a smaller, more efficient model.\"),\\n    (\"Dr. Rachel Kim\", \"That\\'s a great point, Eric. Thanks again for sharing your insights with us!\"),\\n    (\"Dr. Eric Lee\", \"Thanks, Rachel! It was a pleasure being on the show.\"),\\n    (\"Dr. Rachel Kim\", \"And that\\'s all the time we have for today. Thanks to our listeners for tuning in to \\'The AI Frontier\\' podcast. Don\\'t forget to subscribe and join the conversation on our social media channels. See you in the next episode!\"),\\n    (\"Dr. Eric Lee\", \"[sigh] I\\'m glad I could share my knowledge with your listeners.\"),\\n    (\"Dr. Rachel Kim\", \"It was great having you on the show, Dr. Lee. Thanks again for sharing your expertise with us!\"),\\n    (\"Dr. Eric Lee\", \"Thanks, Rachel! It was a pleasure being on the show.\"),\\n    (\"Dr. Rachel Kim\", \"And to our listeners, thanks for tuning in to \\'The AI Frontier\\' podcast. Don\\'t forget to subscribe and join the conversation on our social media channels.\"),\\n    (\"Dr. Eric Lee\", \"I think knowledge distillation has the potential to improve the accuracy and efficiency of AI models, by enabling the transfer of knowledge from a complex model to a smaller, more efficient model.\"),\\n    (\"Dr. Rachel Kim\", \"That\\'s a great point, Eric. Well, it\\'s been a pleasure having you on the show. Thanks again for sharing your expertise with us!\"),\\n    (\"Dr. Eric Lee\", \"Thanks, Rachel! It was a pleasure being on the show.\"),\\n    (\"Dr. Rachel Kim\", \"And to our listeners, thanks for tuning in to \\'The AI Frontier\\' podcast. Don\\'t forget to subscribe and join the conversation on our social media channels.\"),\\n    (\"Dr. Eric Lee\", \"[laughs] I\\'m glad I could share my insights with your listeners.\"),\\n    (\"Dr. Rachel Kim\", \"It was great having you on the show, Dr. Lee. Thanks again for sharing your expertise with us!\"),\\n    (\"Dr. Eric Lee\", \"Thanks, Rachel! It was a pleasure being on the show.\"),\\n    (\"Dr. Rachel Kim\", \"And to our listeners, thanks for tuning in to \\'The AI Frontier\\' podcast. Don\\'t forget to subscribe and join the conversation on our social media channels.\"),\\n    (\"Dr. Eric Lee\", \"I think knowledge distillation has the potential to revolutionize the field of AI, by making advanced capabilities more accessible to a broader range of users.\"),\\n]'}\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d3db7-5bfa-4143-9d4f-db87f22870c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
