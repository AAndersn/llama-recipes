{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f68aee84-04e3-4cbc-be78-6de9e06e704f",
   "metadata": {},
   "source": [
    "Notebook for uploading PDF, extracting all Text and Pre-Processing using a 1B or 3B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4fc7aef-3505-482e-a998-790b8b9d48e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyPDF2\n",
    "#!pip install rich ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60d0061b-8b8c-4353-850f-f19466a0ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = './2402.13116v3.pdf'\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "#DEFAULT_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\" <- Don't think this would be necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9418ac5e-df65-4c03-ac64-48a1275afa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import HtmlDiff\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21029232-ac5f-42ca-b26b-baad5b2f49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import PyPDF2\n",
    "from typing import Optional\n",
    "import os\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "153d9ece-37a4-4fff-a8e8-53f923a2b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_pdf(file_path: str) -> bool:\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return False\n",
    "    if not file_path.lower().endswith('.pdf'):\n",
    "        print(\"Error: File is not a PDF\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b57c2d64-3d75-4aeb-b4ee-bd1661286b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path: str, max_chars: int = 100000) -> Optional[str]:\n",
    "    if not validate_pdf(file_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            # Create PDF reader object\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            # Get total number of pages\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            print(f\"Processing PDF with {num_pages} pages...\")\n",
    "            \n",
    "            extracted_text = []\n",
    "            total_chars = 0\n",
    "            \n",
    "            # Iterate through all pages\n",
    "            for page_num in range(num_pages):\n",
    "                # Extract text from page\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text = page.extract_text()\n",
    "                \n",
    "                # Check if adding this page's text would exceed the limit\n",
    "                if total_chars + len(text) > max_chars:\n",
    "                    # Only add text up to the limit\n",
    "                    remaining_chars = max_chars - total_chars\n",
    "                    extracted_text.append(text[:remaining_chars])\n",
    "                    print(f\"Reached {max_chars} character limit at page {page_num + 1}\")\n",
    "                    break\n",
    "                \n",
    "                extracted_text.append(text)\n",
    "                total_chars += len(text)\n",
    "                print(f\"Processed page {page_num + 1}/{num_pages}\")\n",
    "            \n",
    "            final_text = '\\n'.join(extracted_text)\n",
    "            print(f\"\\nExtraction complete! Total characters: {len(final_text)}\")\n",
    "            return final_text\n",
    "            \n",
    "    except PyPDF2.PdfReadError:\n",
    "        print(\"Error: Invalid or corrupted PDF file\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0984bb1e-d52c-4cec-a131-67a48061fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PDF metadata\n",
    "def get_pdf_metadata(file_path: str) -> Optional[dict]:\n",
    "    if not validate_pdf(file_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            metadata = {\n",
    "                'num_pages': len(pdf_reader.pages),\n",
    "                'metadata': pdf_reader.metadata\n",
    "            }\n",
    "            return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63848943-79cc-4e21-8396-6eab5df493e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata...\n",
      "\n",
      "PDF Metadata:\n",
      "Number of pages: 44\n",
      "Document info:\n",
      "/Author: \n",
      "/CreationDate: D:20240311015030Z\n",
      "/Creator: LaTeX with hyperref\n",
      "/Keywords: \n",
      "/ModDate: D:20240311015030Z\n",
      "/PTEX.Fullbanner: This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5\n",
      "/Producer: pdfTeX-1.40.25\n",
      "/Subject: \n",
      "/Title: \n",
      "/Trapped: /False\n",
      "\n",
      "Extracting text...\n",
      "Processing PDF with 44 pages...\n",
      "Processed page 1/44\n",
      "Processed page 2/44\n",
      "Processed page 3/44\n",
      "Processed page 4/44\n",
      "Processed page 5/44\n",
      "Processed page 6/44\n",
      "Processed page 7/44\n",
      "Processed page 8/44\n",
      "Processed page 9/44\n",
      "Processed page 10/44\n",
      "Processed page 11/44\n",
      "Processed page 12/44\n",
      "Processed page 13/44\n",
      "Processed page 14/44\n",
      "Processed page 15/44\n",
      "Processed page 16/44\n",
      "Reached 100000 character limit at page 17\n",
      "\n",
      "Extraction complete! Total characters: 100016\n",
      "\n",
      "Preview of extracted text (first 500 characters):\n",
      "--------------------------------------------------\n",
      "1\n",
      "A Survey on Knowledge Distillation of Large\n",
      "Language Models\n",
      "Xiaohan Xu1, Ming Li2, Chongyang Tao3, Tao Shen4, Reynold Cheng1, Jinyang Li1,\n",
      "Can Xu5, Dacheng Tao6, Tianyi Zhou2\n",
      "1The University of Hong Kong2University of Maryland3Microsoft\n",
      "4University of Technology Sydney5Peking University6The University of Sydney\n",
      "{shawnxxh,chongyangtao,hishentao }@gmail.com {minglii,tianyi }@umd.edu\n",
      "ckcheng@cs.hku.hk jl0725@connect.hku.hk\n",
      "Abstract â€”In the era of Large Language Models (LLMs), Knowledge Distillati\n",
      "--------------------------------------------------\n",
      "\n",
      "Total characters extracted: 100016\n",
      "\n",
      "Extracted text has been saved to extracted_text.txt\n"
     ]
    }
   ],
   "source": [
    "# Extract metadata first\n",
    "print(\"Extracting metadata...\")\n",
    "metadata = get_pdf_metadata(pdf_path)\n",
    "if metadata:\n",
    "    print(\"\\nPDF Metadata:\")\n",
    "    print(f\"Number of pages: {metadata['num_pages']}\")\n",
    "    print(\"Document info:\")\n",
    "    for key, value in metadata['metadata'].items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Extract text\n",
    "print(\"\\nExtracting text...\")\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Display first 500 characters of extracted text as preview\n",
    "if extracted_text:\n",
    "    print(\"\\nPreview of extracted text (first 500 characters):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(extracted_text[:500])\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"\\nTotal characters extracted: {len(extracted_text)}\")\n",
    "\n",
    "# Optional: Save the extracted text to a file\n",
    "if extracted_text:\n",
    "    output_file = 'extracted_text.txt'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(extracted_text)\n",
    "    print(f\"\\nExtracted text has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c0828a5-964d-475e-b5f5-40a04e287725",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "SYS_PROMPT = \"\"\"\n",
    "You are a world class text pre-processor, here is the raw data from a PDF, please parse and return it in a way that is crispy and usable to send to a podcast writer.\n",
    "\n",
    "The raw data is messed up with new lines, Latex math and you will see fluff that we can remove completely. Basically take away any details that you think might be useless in a podcast author's transcript.\n",
    "\n",
    "Remember, the podcast could be on any topic whatsoever so the issues listed above are not exhaustive\n",
    "\n",
    "The goal is to use this in a podcast research transcript so a lot of the emails, citations, and things like that can be removed-please be smart with what you remove and be creative ok?\n",
    "\n",
    "Remember DO NOT START SUMMARIZING THIS, YOU ARE ONLY CLEANING UP THE TEXT AND RETURNING AS IS\n",
    "\n",
    "Be very smart and aggressive with removing details, you will get a running portion of the text and keep returning the processed text.\n",
    "\n",
    "ALWAYS start your response directly with processed text and NO ACKNOWLEDGEMENTS about my questions ok?\n",
    "Here is the text:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d04a4f07-b0b3-45ca-8f41-a433e1abe050",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    device_map=device,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "model, tokenizer = accelerator.prepare(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbda5241-e890-4402-87dd-514d6761bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(text_chunk, chunk_num):\n",
    "    \"\"\"Process a chunk of text and return both input and output for verification\"\"\"\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": text_chunk},\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            max_new_tokens=512\n",
    "        )\n",
    "    \n",
    "    processed_text = tokenizer.decode(output[0], skip_special_tokens=True)[len(prompt):].strip()\n",
    "    \n",
    "    # Print chunk information for monitoring\n",
    "    #print(f\"\\n{'='*40} Chunk {chunk_num} {'='*40}\")\n",
    "    print(f\"INPUT TEXT:\\n{text_chunk[:500]}...\")  # Show first 500 chars of input\n",
    "    print(f\"\\nPROCESSED TEXT:\\n{processed_text[:500]}...\")  # Show first 500 chars of output\n",
    "    print(f\"{'='*90}\\n\")\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0183c47-339d-4041-ae83-77fc34931075",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"./extracted_text.txt\"  # Replace with your file path\n",
    "CHUNK_SIZE = 1000  # Adjust chunk size if needed\n",
    "\n",
    "# Read the file\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Calculate number of chunks\n",
    "num_chunks = (len(text) + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
    "\n",
    "# Cell 6: Process the file with ordered output\n",
    "# Create output file name\n",
    "output_file = f\"clean_{os.path.basename(INPUT_FILE)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7917dfdd-b3af-44fc-a8c0-2760ace9363e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982a87c01ac14a5187843c211d6add24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chunks:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "1\n",
      "A Survey on Knowledge Distillation of Large\n",
      "Language Models\n",
      "Xiaohan Xu1, Ming Li2, Chongyang Tao3, Tao Shen4, Reynold Cheng1, Jinyang Li1,\n",
      "Can Xu5, Dacheng Tao6, Tianyi Zhou2\n",
      "1The University of Hong Kong2University of Maryland3Microsoft\n",
      "4University of Technology Sydney5Peking University6The University of Sydney\n",
      "{shawnxxh,chongyangtao,hishentao }@gmail.com {minglii,tianyi }@umd.edu\n",
      "ckcheng@cs.hku.hk jl0725@connect.hku.hk\n",
      "Abstract â€”In the era of Large Language Models (LLMs), Knowledge Distillati...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Tao Shen4, Reynold Cheng1, Jinyang Li1,\n",
      "Can Xu5, Dacheng Tao6, Tianyi Zhou2\n",
      "1The University of Hong Kong2University of Maryland3Microsoft\n",
      "4University of Technology Sydney5Peking University6The University of Sydney\n",
      "{shawnxxh,chongyangtao,hishentao }@gmail.com {minglii,tianyi }@umd.edu\n",
      "ckcheng@cs.hku.hk...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "ed knowledge to smaller models and its utility in model compression and self-\n",
      "improvement. Our survey is meticulously structured around three foundational pillars: algorithm ,skill, and verticalization â€“ providing\n",
      "a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications\n",
      "across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, illustrating how\n",
      "DA emerges as a powerfu...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ulously structured around three foundational pillars: algorithm, skill, and verticalization â€“ providing a comprehensive examination of knowledge distillation mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and knowledge distillation, illustrating how DA emerges as a powerful paradigm within the knowledge distillation framework to bolster larg...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "on and\n",
      "proposing future research directions. By bridging the gap between proprietary and open-source LLMs, this survey underscores the\n",
      "potential for more accessible, efficient, and powerful AI solutions. Most importantly, we firmly advocate for compliance with the legal\n",
      "terms that regulate the use of LLMs, ensuring ethical and lawful application of KD of LLMs. An associated Github repository is available\n",
      "at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.\n",
      "Index Terms â€”Large lang...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ce LLMs, this survey underscores the potential for more accessible, efficient, and powerful AI solutions. Most importantly, we firmly advocate for compliance with the legal terms that regulate the use of LLMs, ensuring ethical and lawful application of knowledge distillation. An associated Github repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.\n",
      "\n",
      "Index Terms â€”Large language models, knowledge distillation, data augmentation, skill distillation, supervise...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      " have un-\n",
      "locked new realms of possibility, from generating human-\n",
      "like text to offering sophisticated problem-solving capa-\n",
      "bilities. The core significance of these LLMs lies in their\n",
      "emergent abilities (Wei et al., 2022a,b; Xu et al., 2024a), a\n",
      "phenomenon where the models display capabilities beyond\n",
      "their explicit training objectives, enabling them to tackle a\n",
      "diverse array of tasks with remarkable proficiency. Their\n",
      "deep understanding of context, nuance, and the intrica-\n",
      "cies of human languag...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ergent** abilities, a phenomenon where the models display capabilities beyond their explicit training objectives, enabling them to tackle a diverse array of tasks with remarkable proficiency. Their **deep** understanding of context, nuance, and intricacies of human language enables them to excel in a wide array of applications, from creative content generation to complex problem-solving....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "g to revolutionize industries,\n",
      "augment human creativity, and redefine our interaction with\n",
      "technology.\n",
      "Despite the remarkable capabilities of proprietary LLMs\n",
      "like GPT-4 and Gemini, they are not without their shortcom-\n",
      "ings, particularly when viewed in light of the advantages\n",
      "offered by open-source models. A significant drawback is\n",
      "their limited accessibility and higher cost (OpenAI et al.,\n",
      "2023). These proprietary models often come with substantial\n",
      "usage fees and restricted access, making them ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ogy.\n",
      "Despite the remarkable capabilities of proprietary LLMs like GPT-4 and Gemini, they are not without their shortcomings, particularly when viewed in light of the advantages offered by open-source models.\n",
      "A significant drawback is their limited accessibility and higher cost (OpenAI et al., 2023). These models often come with substantial usage fees and restricted access, making them less attainable for individuals and smaller organizations.\n",
      "In terms of data privacy and security, using these pr...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "straints of accessibility, cost, and adaptability\n",
      "thus present significant challenges in leveraging the full\n",
      "potential of proprietary LLMs.\n",
      "In contrast to proprietary LLMs, open-source modelsarXiv:2402.13116v3  [cs.CL]  8 Mar 2024\n",
      "2\n",
      "like LLaMA (Touvron et al., 2023) and Mistral (Jiang et al.,\n",
      "2023a) bring several notable advantages. One of the primary\n",
      "benefits of open-source models is their accessibility and\n",
      "adaptability. Without the constraints of licensing fees or\n",
      "restrictive usage policies, t...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "aging the full\n",
      "potential of proprietary LLMs. In contrast to proprietary LLMs, open-source models\n",
      "arXiv:2402.13116v3  [cs.CL]  8 Mar 2024\n",
      "2\n",
      "like LLaMA (Touvron et al., 2023) and Mistral (Jiang et al., 2023a) bring several notable advantages. One of the primary benefits of open-source models is their accessibility and adaptability. Without the constraints of licensing fees or restrictive usage policies, these models are more readily available to a broader range of users, from individual researche...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "y stemming from their relatively\n",
      "limited scale and resources compared to their proprietary\n",
      "counterparts. One of the most significant limitations is\n",
      "the smaller model scale, which often results in lower per-\n",
      "formance on real-world tasks with a bunch of instruc-\n",
      "tions (Zheng et al., 2023a). These models, with fewer pa-\n",
      "rameters, may struggle to capture the depth and breadth\n",
      "of knowledge embodied in larger models like GPT-4. Ad-\n",
      "ditionally, the pre-training investment in these open-source\n",
      "models is...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "parts. One of the most significant limitations is the smaller model scale, which often results in lower performance on real-world tasks with a bunch of instructions (Zheng et al., 2023a). These models, with fewer parameters, may struggle to capture the depth and breadth of knowledge embodied in larger models like GPT-4. Additionally, the pre-training investment in these open-source models is typically less substantial. This reduced investment can lead to a narrower range of pre-training data, po...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "ized applications. This\n",
      "limitation becomes particularly evident when these models\n",
      "are compared to the highly fine-tuned proprietary LLMs,\n",
      "which are often tailored to excel in a wide array of complex\n",
      "scenarios (OpenAI et al., 2023).\n",
      "Primarily, recognizing the disparities between propri-\n",
      "etary and open-source LLMs, KD techniques have surged\n",
      "as a means to bridge the performance gap between these\n",
      "models (Gou et al., 2021; Gupta and Agrawal, 2022). Knowl-\n",
      "edge distillation, in this context, involves ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "===========================\n",
      "\n",
      "When it comes to recognizing the differences between proprietary and open-source LLMs, it becomes particularly evident when comparing them to highly fine-tuned proprietary models. These models are often tailored to excel in a wide array of complex scenarios.\n",
      "\n",
      "**The Limitation of Proprietary Models**\n",
      "------------------------------------\n",
      "\n",
      "Recognizing the disparities between proprietary and open-source LLMs is crucial. The fine-tuning process for proprietary models is a...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "t al., 2021) has emerged as a\n",
      "prevalent paradigm to achieve knowledge distillation of\n",
      "LLMs, where a small seed of knowledge is used to prompt\n",
      "the LLM to generate more data with respect to a specific\n",
      "skill or domain (Taori et al., 2023). Secondly, KD still retains\n",
      "its fundamental role in compressing LLMs, making them\n",
      "more efficient without significant loss in performance. (Gu\n",
      "et al., 2024; Agarwal et al., 2024). More recently, the strategy\n",
      "of employing open-source LLMs as teachers for their own\n",
      "s...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ed of knowledge is used to prompt the LLM to generate more data with respect to a specific skill or domain. Secondly, KD retains its fundamental role in compressing LLMs, making them more efficient without significant loss in performance. (Gu et al., 2024; Agarwal et al., 2024). More recently, the strategy of employing open-source LLMs as teachers for their own self-improvement has emerged as a promising approach, enhancing their capabilities significantly. Figure 1 provides an illustration of t...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "\n",
      "via self-generated knowledge.\n",
      "A key aspect of the knowledge distillation is the en-\n",
      "hancement of skills such as advanced context following\n",
      "(e.g., in-context learning (Huang et al., 2022a) and in-\n",
      "struction following (Taori et al., 2023)), improved align-\n",
      "ment with user intents (e.g., human values/principles (Cui\n",
      "et al., 2023a), and thinking patterns like chain-of-thought\n",
      "(CoT) (Mukherjee et al., 2023)), and NLP task specialization\n",
      "(e.g., semantic understanding (Ding et al., 2023a), and code\n",
      "gen...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "standing\n",
      "and in-context learning\n",
      "improved alignment with user intents\n",
      "human values and principles\n",
      "thinking patterns like chain-of-thought\n",
      "and NLP task specialization\n",
      "semantic understanding\n",
      "code generation\n",
      "these skills are crucial for a wide range of applications\n",
      "from casual conversations to complex problem-solving\n",
      "in specialized domains\n",
      "in vertical domains like healthcare\n",
      "law\n",
      "science\n",
      "where accuracy and contextual knowledge are paramount\n",
      "knowledge distillation enables open-source models to improv...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "rom the\n",
      "proprietary models that have been extensively trained and\n",
      "fine-tuned in these areas.\n",
      "The benefits of knowledge distillation in the era of\n",
      "LLMs are multifaceted and transformative (Gu et al., 2024).\n",
      "Through a suite of distillation techniques, the gap between\n",
      "proprietary and open-source models is significantly nar-\n",
      "rowed (Chiang et al., 2023; Xu et al., 2023a) and even\n",
      "filled (Zhao et al., 2023a). This process not only streamlines\n",
      "computational requirements but also enhances the environ-\n",
      "m...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "of knowledge distillation in the era of LLMs are multifaceted and transformative.\n",
      "Through a suite of distillation techniques, the gap between proprietary and open-source models is significantly narrowed.\n",
      "This process streamlines computational requirements and enhances environmental sustainability of AI operations.\n",
      "Open-source models become more proficient with lesser computational overhead.\n",
      "Furthermore, knowledge distillation fosters an accessible and equitable AI landscape.\n",
      "Smaller entities and...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      " growth across various industries\n",
      "and research domains.\n",
      "The escalating need for a comprehensive survey on the\n",
      "knowledge distillation of LLMs stems from the rapidly\n",
      "evolving landscape of AI (OpenAI et al., 2023; Team et al.,\n",
      "2023) and the increasing complexity of these models. As AI\n",
      "continues to penetrate various sectors, the ability to effi-\n",
      "ciently and effectively distill knowledge from proprietary\n",
      "LLMs to open-source ones becomes not just a technical\n",
      "aspiration but a practical necessity. This ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ey on the\n",
      "knowledge distillation of LLMs stems from the rapidly evolving landscape of AI\n",
      "and the increasing complexity of these models\n",
      "as ai continues to penetrate various sectors the ability to efficiently and effectively distill knowledge from proprietary lls to open-source ones becomes not just a technical aspiration but a practical necessity...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "eRankOptimizationy,1y,2y3y1y2y3â‰»â‰»rankâ€¦â€¦\n",
      "DataCuration\n",
      "X,YrawdatasynthesizefeedbackFeedback\n",
      "input\n",
      "outputSelf-Knowledge\n",
      "outputinputinput\n",
      "YlabelLabelingExpansion\n",
      "X,YdemonstrationsexpandFeature\n",
      "featureinput,outputextractSec.4Sec.5\n",
      "Sec.3.1Sec.3.2\n",
      "Fig. 2: An overview of this survey on knowledge distillation of large language models. Note that â€˜Sectionâ€™ is abbreviated\n",
      "as â€˜Sec.â€™ in this figure. RM S(Â·)denotes the student reward model.\n",
      "the growing demand for more accessible, cost-effective, and\n",
      "adaptable ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "utSelf-Knowledge\n",
      "labelingExpansion\n",
      "X,YdemonstrationsexpandFeature\n",
      "featureinput,outputextractSec.4Sec.5\n",
      "Sec.3.1Sec.3.2\n",
      "Fig. 2: An overview of this survey on knowledge distillation of large language models. Note that â€˜Sectionâ€™ is abbreviated\n",
      "as â€˜Sec.â€™ in this figure. RM S(Â·)denotes the student reward model.\n",
      "the growing demand for more accessible, cost-effective, and\n",
      "adaptable AI solutions that can cater to a diverse range\n",
      "of applications and users. A survey in this field is vital\n",
      "for synthesizing ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "posing direc-\n",
      "tions for future research.\n",
      "Survey Organization. The remainder of this survey is orga-\n",
      "nized into several comprehensive sections, each designed to\n",
      "offer a deep dive into the multifaceted aspects of knowledge\n",
      "distillation within the realm ofLLMs. Following this intro-\n",
      "duction, Â§2 provides a foundational overview of knowledge\n",
      "distillation, comparing traditional techniques with those\n",
      "emerging in the era of LLMs and highlighting the role of\n",
      "data augmentation (DA) in this context. Â§3 del...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "s organized into several comprehensive sections, each designed to offer a deep dive into the multifaceted aspects of knowledge distillation within the realm of LLMs. Following this introduction, Â§2 provides a foundational overview of knowledge distillation, comparing traditional techniques with those emerging in the era of LLMs and highlighting the role of data augmentation (DA) in this context. Â§3 delves into the approaches to elicit knowledge from teacher LLMs and core distillation algorithms,...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "guage understanding (NLU), genera-\n",
      "tion (NLG), information retrieval, recommendation systems,\n",
      "and the evaluation of text generation. In Â§5, we ventureinto domain-specific vertical distillation, showcasing how\n",
      "knowledge distillation techniques are applied within spe-\n",
      "cialized fields such as law, healthcare, finance, and science,\n",
      "illustrating the practical implications and transformative\n",
      "impact of these approaches. The survey suggests open\n",
      "problems in Â§6, identifying current challenges and gaps in...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "lves the process of identifying, selecting, and combining the most relevant knowledge from a dataset to create a distilled representation of the original information....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "large, complex model (teacher) to a\n",
      "smaller, more efficient model (student) (Gou et al., 2021).\n",
      "This technique is pivotal in mitigating the challenges posed\n",
      "by the computational demands and resource constraints of\n",
      "deploying large-scale models in practical applications.\n",
      "Historically, knowledge distillation techniques, prior to\n",
      "the era of LLMs, primarily concentrated on transferring\n",
      "knowledge from complex, often cumbersome neural net-\n",
      "works to more compact and efficient architectures (Sanh\n",
      "et al.,...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ue is pivotal in mitigating the challenges posed\n",
      "by the computational demands and resource constraints of\n",
      "deploying large-scale models in practical applications.\n",
      "Historically, knowledge distillation techniques, prior\n",
      "to the era of LLMs, primarily concentrated on transferring\n",
      "knowledge from complex, often cumbersome neural net-\n",
      "works to more compact and efficient architectures (Sanh\n",
      "et al., 2019; Kim and Rush, 2016)....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      " (Chenglin et al., 2023)\n",
      "ExpansionSelf-Instruct (Wang et al., 2022a), Alpaca (Taori et al., 2023), Code Alpaca (Chaudhary, 2023)\n",
      "Self-Align (Sun et al., 2024b), WizardLM (Xu et al., 2023a), WizardCoder (Luo et al., 2023a),\n",
      "WizardMath (Luo et al., 2023b), AugGPT (Dai et al., 2023a), TDG (He et al., 2023b)\n",
      "CurationUltraChat (Ding et al., 2023b), Phi-1 (Gunasekar et al., 2023), Phi-1.5 (Li et al., 2023a),\n",
      "Phi-2 (Mar, 2023), Magicoder (Wei et al., 2023), WaveCoder (Yu et al., 2024)\n",
      "ZeroGen (Ye et al...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Code Alpaca (Chaudhary, 2023)\n",
      "Self-Align (Sun et al., 2024b), WizardLM (Xu et al., 2023a), WizardCoder (Luo et al., 2023a),\n",
      "WizardMath (Luo et al., 2023b), AugGPT (Dai et al., 2023a), TDG (He et al., 2023b)\n",
      "CurationUltraChat (Ding et al., 2023b), Phi-1 (Gunasekar et al., 2023), Phi-1.5 (Li et al., 2023a),\n",
      "Phi-2 (Mar, 2023), Magicoder (Wei et al., 2023), WaveCoder (Yu et al., 2024)\n",
      "ZeroGen (Ye et al., 2022), SunGen (Gao et al., 2023a), InPars (Bonifacio et al., 2022)\n",
      "FeatureBabyLlama (Timiryasov ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "2024)\n",
      "Self-KnowledgeSelf-Instruct (Wang et al., 2022a), Self-Align (Sun et al., 2024b), RLCD (Yang et al., 2024a),\n",
      "ImpDistill (Jung et al., 2023), LMSI (Huang et al., 2023a), ReST (Gulcehre et al., 2023),\n",
      "Self-Rewarding (Yuan et al., 2024a), Baize (Xu et al., 2023b), STaR (Zelikman et al., 2022)\n",
      "DistillationSupervised Fine-TuningAlpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), WizardLM (Xu et al., 2023a),\n",
      "Self-Instruct (Wang et al., 2022a), Baize (Xu et al., 2023b), STaR (Zelikman et a...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "et al., 2024a), \n",
      "ImpDistill (Jung et al., 2023), LMSI (Huang et al., 2023a), \n",
      "ReST (Gulcehre et al., 2023), Self-Rewarding (Yuan et al., 2024a), Baize (Xu et al., 2023b), \n",
      "STaR (Zelikman et al., 2022), \n",
      "DistillationSupervised Fine-TuningAlpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), \n",
      "WizardLM (Xu et al., 2023a), \n",
      "Self-Instruct (Wang et al., 2022a), Baize (Xu et al., 2023b), STaR (Zelikman et al., 2022), \n",
      "Divergence and SimilarityDistilGPT (Sanh et al., 2019), f-Distill (Wen et al., 2...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "kill\n",
      "DistillationContext FollowingInstruction FollowingSelf-Instruct (Wang et al., 2022a), Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023),\n",
      "WizardLM (Xu et al., 2023a), Orca (Mukherjee et al., 2023), Orca 2 (Mitra et al., 2023),\n",
      "WizardMath (Luo et al., 2023b), Llama-GPT4 (Peng et al., 2023a),\n",
      "Multi-turn DialogueVicuna (Chiang et al., 2023), Baize (Xu et al., 2023b), UltraLLaMA (Ding et al., 2023b),\n",
      "CAMEL (Li et al., 2023b), OpenChat (Wang et al., 2023c), Zephyr (Tunstall et al., 2023),...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "et al., 2023), Vicuna (Chiang et al., 2023),\n",
      "WizardLM (Xu et al., 2023a), Orca (Mukherjee et al., 2023), Orca 2 (Mitra et al., 2023),\n",
      "WizardMath (Luo et al., 2023b), Llama-GPT4 (Peng et al., 2023a),\n",
      "Multi-turn DialogueVicuna (Chiang et al., 2023), Baize (Xu et al., 2023b), UltraLLaMA (Ding et al., 2023b),\n",
      "CAMEL (Li et al., 2023b), OpenChat (Wang et al., 2023c), Zephyr (Tunstall et al., 2023),\n",
      "RAG Capbility KARD (Kang et al., 2023a), SAIL (Luo et al., 2023c), Self-RAG (Asai et al., 2023),\n",
      "Alignme...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "2023), UltraFeedback (Cui et al., 2023a),\n",
      "ValueCAI (Bai et al., 2022a), Align Honesty (Yang et al., 2023a), SANDBOX (Liu et al., 2023b),\n",
      "Self-Align (Sun et al., 2024b), UltraFeedback (Cui et al., 2023a), RLCD (Yang et al., 2024a)\n",
      "AgentTool UsingToolformer (Schick et al., 2023), Graph-ToolFormer (Zhang, 2023), Gorilla (Patil et al., 2023),\n",
      "ToolAlpaca (Tang et al., 2023a), ToolLLM (Qin et al., 2023a), CRAFT (Yuan et al., 2023a),\n",
      "Confucius (Gao et al., 2023b), MLLM-Tool (Wang et al., 2024), Î±-UMi (...\n",
      "\n",
      "PROCESSED TEXT:\n",
      ", 2023a), SANDBOX (Liu et al., 2023b),\n",
      "Self-Align (Sun et al., 2024b), UltraFeedback (Cui et al., 2023a), RLCD (Yang et al., 2024a)\n",
      "AgentToolformer (Schick et al., 2023), Graph-ToolFormer (Zhang, 2023), Gorilla (Patil et al., 2023),\n",
      "ToolAlpaca (Tang et al., 2023a), ToolLLM (Qin et al., 2023a), CRAFT (Yuan et al., 2023a),\n",
      "Confucius (Gao et al., 2023b), MLLM-Tool (Wang et al., 2024), Î±-UMi (Shen et al., 2024),\n",
      "PlanningFireAct (Chen et al., 2023b), AgentTuning (Zeng et al., 2023a), Lumos (Yin et al...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "OMP (Xu et al., 2024b), MaRio (Ramnath et al., 2023),\n",
      "ID (Jung et al., 2023), GPT-3 Labeling (Wang et al., 2021b), BioGPT (Guo et al., 2023a),\n",
      "ChatGPT NMT (Yang and Nicolai, 2023),\n",
      "Information RetrievalQUILL (Srinivasan et al., 2022), Promptgator (Dai et al., 2023b), InPars (Bonifacio et al., 2022),\n",
      "AugTriever (Meng et al., 2023), (Sun et al., 2023a), RankVicuna (Pradeep et al., 2023a),\n",
      "RankZephyr (Pradeep et al., 2023b), ExaRanker (Ferraretto et al., 2023),\n",
      "Recommendation NDR (Mysore et al., 20...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "l., 2021b), BioGPT (Guo et al., 2023a),\n",
      "ChatGPT NMT (Yang and Nicolai, 2023),\n",
      "Information RetrievalQUILL (Srinivasan et al., 2022), Promptgator (Dai et al., 2023b), InPars (Bonifacio et al., 2022),\n",
      "AugTriever (Meng et al., 2023), (Sun et al., 2023a), RankVicuna (Pradeep et al., 2023a),\n",
      "RankZephyr (Pradeep et al., 2023b), ExaRanker (Ferraretto et al., 2023),\n",
      "Recommendation NDR (Mysore et al., 2023), InstrcutRec (Zhang et al., 2023b), ONCE (Liu et al., 2023c),\n",
      "Text Generation EvaluationPandaLM (Wa...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "lti-ModalityLLaVA (Liu et al., 2023e), SVIT (Zhao et al., 2023b), LVIS-Instruct4V (Wang et al., 2023e), Shikra (Chen et al., 2023c),\n",
      "LSKD (Park et al., 2023), DetGPT (Pi et al., 2023; Zhao et al., 2023c), LRV (Liu et al., 2023f), NExT-GPT (Wu et al., 2023b),\n",
      "Valley (Luo et al., 2023d), ILuvUI (Jiang et al., 2023d), StableLLaVA (Li et al., 2023c), PointLLM (Xu et al., 2023e),\n",
      "Verticalization\n",
      "DistillationLaw (Huang et al., 2023b; Cui et al., 2023b); Medical & Healthcare (Zhang et al., 2023c; Chen ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "23e), Shikra (Chen et al., 2023c),\n",
      "LSKD (Park et al., 2023), DetGPT (Pi et al., 2023; Zhao et al., 2023c), LRV (Liu et al., 2023f), NExT-GPT (Wu et al., 2023b),\n",
      "Valley (Luo et al., 2023d), ILuvUI (Jiang et al., 2023d), StableLLaVA (Li et al., 2023c), PointLLM (Xu et al., 2023e),\n",
      "Verticalization\n",
      "DistillationLaw (Huang et al., 2023b; Cui et al., 2023b); Medical & Healthcare (Zhang et al., 2023c; Chen et al., 2023d); Finance (Zhang and Yang, 2023)...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "network to mimic the\n",
      "output of a larger teacher network, often through techniques\n",
      "like soft target training, where the student learns from\n",
      "the softened softmax output of the teacher. Please refer to\n",
      "the survey (Gou et al., 2021) for more details on general\n",
      "knowledge distillation techniques in AI and DL.\n",
      "In contrast, the advent of LLMs has revolutionized\n",
      "the knowledge distillation landscape. The current era of\n",
      "knowledge distillation in LLMs shifts the focus from mere\n",
      "architecture compression to t...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "training, where the student learns from the softened softmax output of the teacher.\n",
      "refer to the survey (Gou et al., 2021) for more details on general knowledge distillation techniques in AI and DL.\n",
      "In contrast, the advent of LLMs has revolutionized\n",
      "the knowledge distillation landscape.\n",
      "The current era of knowledge distillation in LLMs shifts the focus from mere\n",
      "architecture compression to the more nuanced process of knowledge elicitation and transfer\n",
      "Taori et al., 2023; Chaudhary, 2023; Tunstal...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "r reduce the model size , the current focus in LLM-based\n",
      "knowledge distillation is to extract and transfer the rich,\n",
      "nuanced understanding that these models have developed.\n",
      "The key to this modern approach lies in heuristic and\n",
      "carefully designed prompts, which are used to elicit specific\n",
      "knowledge (Ding et al., 2023b) or capabilities (Chaudhary,\n",
      "2023) from the LLMs. These prompts are crafted to tap\n",
      "into the LLMâ€™s understanding and capabilities in various\n",
      "domains, ranging from natural language un...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "tract nuanced understanding from LLMs.\"...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "their explicit training objectives.\n",
      "Furthermore, this era of knowledge distillation also em-\n",
      "phasizes the transfer of more abstract qualities such as\n",
      "reasoning patterns (Mitra et al., 2023), preference align-\n",
      "ment (Cui et al., 2023a), and value alignment (Sun et al.,\n",
      "2024b). This is in stark contrast to the earlier focus on output\n",
      "replication (Taori et al., 2023), indicating a shift towards\n",
      "a more holistic and comprehensive transfer of cognitive\n",
      "capabilities. The current techniques involve not j...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ch as \n",
      "reasoning patterns, preference alignment, and value alignment. This shift towards a more holistic and comprehensive transfer of cognitive capabilities. The current techniques involve not just the replication of outputs, but also the emulation of thought processes and decision-making patterns of the teacher model. This involves complex strategies like chain-of-thought prompting, where the student model is trained to learn the reasoning process of the teacher, thereby enhancing its problem-...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      " al., 2022) emerges as a critical paradigm integral\n",
      "to the process of knowledge distillation. Unlike traditional\n",
      "DA techniques such as paraphrasing (Gangal et al., 2022) orback-translation (Longpre et al., 2019), which primarily aim\n",
      "at expanding the training dataset in a somewhat mechanical\n",
      "manner. DA within the context of LLMs focuses on the\n",
      "generation of novel, context-rich training data tailored to\n",
      "specific domains and skills. This innovation is driven by the\n",
      "unique capabilities of LLMs to ge...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "e traditional\n",
      "DA techniques such as paraphrasing (Gangal et al., 2022) or back-translation (Longpre et al., 2019), which primarily aim\n",
      "at expanding the training dataset in a somewhat mechanical\n",
      "manner. DA within the context of LLMs focuses on the\n",
      "generation of novel, context-rich training data tailored to\n",
      "specific domains and skills. This innovation is driven by the\n",
      "unique capabilities of LLMs to generate coherent, diverse,\n",
      "and intricate data samples that closely mimic the nuanced\n",
      "understanding ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "capability gap between proprietary and open-\n",
      "source models. Through DA, LLMs are prompted to create\n",
      "targeted, high-quality datasets that are not merely larger in\n",
      "volume but are also rich in diversity and specificity. This\n",
      "approach enables the distillation process to be more effec-\n",
      "tive, ensuring that the distilled models not only replicate\n",
      "the teacher modelâ€™s output behavior but also embody its\n",
      "deep-seated understanding and cognitive strategies.\n",
      "The significance and necessity of DA for achieving...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "e targeted, high-quality datasets that are not merely larger in volume but also rich in diversity and specificity. This approach enables the distillation process to be more effective, ensuring that the distilled models replicate the teacher modelâ€™s output behavior and embody its deep-seated understanding and cognitive strategies.\n",
      "\n",
      "DA is a key factor in achieving knowledge discovery in the LLM era, as it enables the distilled models to acquire and refine capabilities that would otherwise require ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "ssible approach to harnessing\n",
      "the power of LLMs. It empowers open-source models with\n",
      "the ability to approximate the contextual adeptness, ethical\n",
      "alignment, and deep semantic insights characteristic of their\n",
      "proprietary counterparts, thereby democratizing access to\n",
      "advanced AI capabilities and fostering innovation across a\n",
      "broader spectrum of applications and users.\n",
      "2.3 Survey Scope\n",
      "Building on the discussions introduced earlier, this survey\n",
      "aims to comprehensively explore the landscape of knowl...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "fields, including but not limited to\n",
      "- Developing AI models that can learn and improve over time\n",
      "- Enhancing human-AI collaboration and interaction\n",
      "- Improving language understanding and generation capabilities\n",
      "- Enabling the creation of high-quality, explainable AI models\n",
      "2.3.1 Survey Scope\n",
      "This survey aims to investigate the current state of knowledge distillation within the context of LLMs\n",
      "in three primary areas:\n",
      "- KD Algorithms\n",
      "- Skill Distillation\n",
      "- Verticalization Distillation\n",
      "Each facet w...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "ions and methodologies of knowledge distillation. It\n",
      "includes an in-depth exploration of the processes involved\n",
      "in constructing knowledge from teacher models (e.g., pro-\n",
      "prietary LLMs) and integrating this knowledge into student\n",
      "models (e.g., open-source LLMs). Under the umbrella of\n",
      "â€˜knowledge â€™, we delve into strategies such as labeling (Hsieh\n",
      "et al., 2023), expansion (Taori et al., 2023), curation (Gu-\n",
      "nasekar et al., 2023), feature understanding (Agarwal et al.,\n",
      "6\n",
      "2024), feedback mechanisms (...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "onstructing knowledge \n",
      "from teacher models (e.g., proprietary LLMs) and integrating this \n",
      "knowledge into student models (e.g., open-source LLMs)\n",
      "strategies such as labeling, expansion, curation, feature \n",
      "understanding, feedback mechanisms, and self-knowledge generation \n",
      "under the umbrella of 'knowledge' we delve into\n",
      "strategies such as supervised fine-tuning, divergence minimization, \n",
      "reinforcement learning techniques, and rank optimization strategies\n",
      "knowledge distillation seeks to uncover vari...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "This analysis\n",
      "aims to illuminate how these algorithms facilitate the trans-\n",
      "fer of knowledge, ensuring that open-source models can\n",
      "replicate and, in some cases, surpass the capabilities of their\n",
      "proprietary counterparts.\n",
      "Skill Distillation. This facet examines the specific compe-\n",
      "tencies and capabilities enhanced through KD. It encom-\n",
      "passes detailed discussions on context following (Taori et al.,\n",
      "2023; Luo et al., 2023c), with subtopics like instruction\n",
      "following and retrieval-augmented generat...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "models to replicate and potentially surpass proprietary counterparts in various capabilities.\n",
      "\n",
      "**Distillation**\n",
      "This facet focuses on the specific strengths and capabilities enhanced through Knowledge Distillation. It includes discussions on context and following (Taori et al., 2023; Luo et al., 2023c), instruction following and retrieval-augmented generation (RAG) capabilities.\n",
      "\n",
      "**Alignment**\n",
      "In the realm of alignment, a survey investigates thinking patterns, persona/preference modeling, and va...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      ", text generation evaluation, and code gen-\n",
      "eration. Finally, the survey addresses multi-modality (Liu\n",
      "et al., 2023e; Zhao et al., 2023b), exploring how KD enhances\n",
      "LLMsâ€™ ability to interpret and integrate multiple forms of\n",
      "input, enriching their utility and applicability across various\n",
      "contexts.\n",
      "Verticalization Distillation. This section assesses the ap-\n",
      "plication of KD across diverse vertical domains, offering\n",
      "insights into how distilled LLMs can be tailored for spe-\n",
      "cialized fields such as La...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "iu et al., 2023; Zhao et al., 2023b), exploring how KD enhances LLM's ability to interpret and integrate multiple forms of input, enriching their utility and applicability across various contexts.\n",
      "Verticalization Distillation. This section assesses the application of KD across diverse vertical domains, offering insights into how distilled LLMs can be tailored for specialized fields such as Law (LAW, 2023), Medical & Healthcare (Wang et al., 2023a), Finance (Zhang and Yang, 2023), Science (Zhang ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "roader AI and ML ecosystem.\n",
      "By navigating through these facets, this survey en-\n",
      "deavors to provide an extensive and nuanced analysis of\n",
      "knowledge distillation in the era of LLMs. It serves as a\n",
      "guide for researchers, practitioners, and enthusiasts in the\n",
      "field, shedding light on current methodologies, challenges,\n",
      "and opportunities for innovation in this rapidly evolving\n",
      "domain.\n",
      "Declaration. This survey represents our earnest effort to\n",
      "provide a comprehensive and insightful overview of knowl-\n",
      "edg...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "n extensive and nuanced analysis of knowledge distillation in the era of LLMs. It serves as a guide for researchers, practitioners, and enthusiasts in the field, shedding light on current methodologies, challenges, and opportunities for innovation in this rapidly evolving domain.\n",
      "Declaration. This survey represents our earnest effort to provide a comprehensive and insightful overview of knowledge distillation techniques applied to LLMs, focusing on algorithms, skill enhancement, and domain-speci...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      " their impacts\n",
      "across a range of applications.\n",
      "2.4 Distillation Pipeline in LLM Era\n",
      "SeedKnowledgeSkill/Domain\n",
      "TeacherLLMKnowledgeElicitationStudentModelDistillationAlgorithmsteer\n",
      "driveGeneratedKnowledgeLearningObjectivetrain\n",
      "Fig. 4: An illustration of a general pipeline to distill knowl-\n",
      "edge from a large language model to a student model.\n",
      "The general distillation pipeline of LLMs is a structured\n",
      "and methodical process aimed at transferring knowledge\n",
      "from a sophisticated teacher model to a less ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "skill or domain to be learned.**\n",
      "\n",
      "their impacts\n",
      "across a range of applications.\n",
      "\n",
      "2.4 Distillation Pipeline in LLM Era\n",
      "SeedKnowledgeSkill/Domain\n",
      "TeacherLLMKnowledgeElicitationStudentModelDistillationAlgorithmsteer\n",
      "driveGeneratedKnowledgeLearningObjectivetrain\n",
      "Fig. 4: An illustration of a general pipeline to distill knowl-\n",
      "edge from a large language model to a student model.\n",
      "The general distillation pipeline of LLMs is a structured\n",
      "and methodical process aimed at transferring knowledge\n",
      "from a soph...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "lves directing the teacher LLM towards a\n",
      "specific target skill or domain. This is achieved through care-\n",
      "fully crafted instructions or templates that guide the LLMâ€™s\n",
      "focus. These instructions are designed to elicit responses\n",
      "that demonstrate the LLMâ€™s proficiency in a particular area,\n",
      "be it a specialized domain like healthcare or law, or a skill\n",
      "such as reasoning or language understanding. The objective\n",
      "here is to utilize the teacher LLMâ€™s extensive training and\n",
      "nuanced capabilities to generate ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "the LLMâ€™s focus\n",
      "elicit responses that demonstrate the LLMâ€™s proficiency\n",
      "in a particular area\n",
      "specialized domain like healthcare or law\n",
      "or a skill\n",
      "such as reasoning or language understanding\n",
      "objective\n",
      "to utilize the teacher LLMâ€™s extensive training and nuanced capabilities\n",
      "to generate outputs that are rich in the specific knowledge or skills desired for the student model\n",
      "seed knowledge\n",
      "typically comprises\n",
      "a small dataset or specific data clues relevant to the elicit\n",
      "skill or domain knowledge from...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      " seed knowledge is crucial as it provides a\n",
      "foundation upon which the teacher model can build and\n",
      "expand, thereby creating more comprehensive and in-depth\n",
      "knowledge examples.\n",
      "III. Generation of Distillation Knowledge. In response\n",
      "to the seed knowledge and steering instructions, the teacher\n",
      "LLM generates knowledge examples. These examples are\n",
      "predominantly in the form of question-and-answer (QA)\n",
      "dialogues or narrative explanations, aligning with the nat-\n",
      "ural language processing/understanding cap...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ately creating a more comprehensive and in-depth knowledge base.\n",
      "III. Distillation of Knowledge. The teacher LLM generates knowledge examples, primarily in the form of question-and-answer dialogues or narrative explanations, aligning with the language processing capabilities of the model.\n",
      "In certain specialized cases, the outputs may include logits or hidden features, although this is less common due to the complexity of the data form.\n",
      "The generated knowledge examples form the core of the distil...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "ge examples to train the student\n",
      "model. This training is guided by a loss function that aligns\n",
      "with the learning objectives. The loss function quantifies\n",
      "the student modelâ€™s performance in replicating or adapting\n",
      "the knowledge from the teacher model. By minimizing this\n",
      "loss, the student model learns to emulate the target skills or\n",
      "domain knowledge of the teacher, thereby acquiring similar\n",
      "capabilities. The process involves iteratively adjusting the\n",
      "student modelâ€™s parameters to reduce the discre...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "xt (o) and combined with the seed knowledge (s) to produce the output (o')...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "ch the LLM can\n",
      "explore to generate novel knowledge, Parse( o, s)stands for\n",
      "to parse the distillation example ( e.g., (x, y)) from the\n",
      "teacher LLMâ€™s output o(plus the input sin some cases),\n",
      "andpTrepresents the teacher LLM with parameters Î¸T.\n",
      "Given the datasets D(kd)\n",
      "Ibuilt for distillation, we then define\n",
      "a learning objective as\n",
      "L=X\n",
      "ILI(D(kd)\n",
      "I;Î¸S), (2)\n",
      "whereP\n",
      "Idenotes there could be multiple tasks or skills\n",
      "being distilled into one student model, LI(Â·;Â·)stands for a\n",
      "specific learning objective, ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "on example ( e.g., (x, y)) from the\n",
      "teacher LLMâ€™s output o(plus the input sin some cases),\n",
      "andpTrepresents the teacher LLM with parameters Î¸T.\n",
      "Given the datasets D(kd)\n",
      "Ibuilt for distillation, we then define\n",
      "a learning objective as\n",
      "L=X\n",
      "ILI(D(kd)\n",
      "I;Î¸S), (2)\n",
      "where...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      " LLMs (Eq.1), and â€˜Distillation,â€™\n",
      "centered on injecting this knowledge into student models\n",
      "(Eq.2). We will elaborate on these two processes in the\n",
      "subsequent sections.\n",
      "3.1 Knowledge\n",
      "This section focuses on the approaches to elicit knowledge\n",
      "from teacher LLMs. According to the manners to acquire\n",
      "knowledge, we divided them into Labeling ,Expansion ,DataCuration ,Feature ,Feedback , and Self-Knowledge . Figure 5\n",
      "shows an illustration of these knowledge elicitation meth-\n",
      "ods.\n",
      "3.1.1 Labeling\n",
      "Labeling...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "aborate on these two processes in the\n",
      "subsequent sections.\n",
      "3.1 Knowledge\n",
      "This section focuses on the approaches to elicit knowledge\n",
      "from teacher LLMs. According to the manners to acquire\n",
      "knowledge, we divided them into Labeling,Expansion,DataCuration,Feature,Feedback, and Self-Knowledge. Figure 5\n",
      "shows an illustration of these knowledge elicitation methods....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "lable through the\n",
      "predefined Iandc. This process can be formulated as\n",
      "follows:\n",
      "D(lab)={x, y|xâˆ¼ X, yâˆ¼pT(y|IâŠ•câŠ•x)}. (3)\n",
      "Input xcould be sourced from existing NLP task\n",
      "datasets, which serve as typical reservoirs for distillation\n",
      "efforts. Numerous works have sought to harness the capa-\n",
      "bilities of powerful LLMs as teachers for annotating dataset\n",
      "samples across a range of tasks. For instance, efforts in\n",
      "natural language understanding involve using LLMs to cat-\n",
      "egorize text (Gilardi et al., 2023; Ding...\n",
      "\n",
      "PROCESSED TEXT:\n",
      ", yâˆ¼pT(y|IâŠ•câŠ•x)}. (3)\n",
      "Input xcould be sourced from existing NLP task\n",
      "datasets, which serve as typical reservoirs for distillation\n",
      "efforts. Numerous works have sought to harness the\n",
      "capabilities of powerful LLMs as teachers for annotating dataset\n",
      "samples across a range of tasks. For instance, efforts in\n",
      "natural language understanding involve using LLMs to\n",
      "categorize text (Gilardi et al., 2023; Ding et al., 2023a; He et al.,\n",
      "2023a), while in natural language generation, LLMs assist\n",
      "in generating s...\n",
      "==========================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_text = \"\"  # Initialize complete processed text\n",
    "with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "    for chunk_num in tqdm(range(num_chunks), desc=\"Processing chunks\"):\n",
    "        # Get chunk with overlap\n",
    "        start_idx = chunk_num * CHUNK_SIZE\n",
    "        end_idx = start_idx + CHUNK_SIZE\n",
    "        \n",
    "        chunk = text[start_idx:end_idx]\n",
    "        \n",
    "        # Process chunk and append to complete text\n",
    "        processed_chunk = process_chunk(chunk, chunk_num)\n",
    "        processed_text += processed_chunk + \"\\n\"\n",
    "        \n",
    "        # Write chunk immediately to file\n",
    "        out_file.write(processed_chunk + \"\\n\")\n",
    "        \n",
    "        # Force flush the file to disk\n",
    "        out_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef51a7-f13f-49a4-8f73-9ac8ce75319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Input file: {INPUT_FILE}\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "print(f\"Total chunks processed: {num_chunks}\")\n",
    "\n",
    "# Preview the beginning and end of the complete processed text\n",
    "print(\"\\nPreview of final processed text:\")\n",
    "print(\"\\nBEGINNING:\")\n",
    "print(processed_text[:1000])\n",
    "print(\"\\n...\\n\\nEND:\")\n",
    "print(processed_text[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b16ae0e-04cf-4eb9-a369-dee1728b89ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
